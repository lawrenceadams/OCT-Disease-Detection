{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803e3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3744ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(3409)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab16073",
   "metadata": {},
   "source": [
    "# 1. Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f43100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "input_shape = 256\n",
    "num_classes = 2\n",
    "classes = ('ARMD', 'Normal')\n",
    "\n",
    "# hyper\n",
    "batch_size =6\n",
    "num_epochs = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "# gpu\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176e818",
   "metadata": {},
   "source": [
    "# 2. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d7f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # randomly select 55 images from class 'normal'\n",
    "# import shutil, random, os\n",
    "# raw_path = './raw/normal'\n",
    "# balance_path = './balanced/normal'\n",
    "# if not os.path.exists(balance_path):\n",
    "#     os.makedirs(balance_path)\n",
    "\n",
    "# def balance_files():\n",
    "#     filenames = random.sample(os.listdir(raw_path), 55)\n",
    "#     for fname in filenames:\n",
    "#         srcpath = os.path.join(raw_path, fname)\n",
    "#         shutil.copy(srcpath, balance_path)\n",
    "#     shutil.copytree('./raw/armd', './balanced/armd', dirs_exist_ok=True)\n",
    "\n",
    "# balanced_normal = len([file for file in os.listdir(balance_path)])\n",
    "# if balanced_normal != 55:\n",
    "#     balance_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f898881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 110 files [00:00, 221.12 files/s]\n"
     ]
    }
   ],
   "source": [
    "# split training images and testing images\n",
    "import splitfolders\n",
    "raw = './number'\n",
    "split = './processed'\n",
    "splitfolders.ratio(raw, split, seed = 1337, ratio = (0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cacb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len([file for file in os.listdir('./processed/train/armd')]))\n",
    "# print(len([file for file in os.listdir('./processed/train/normal')]))\n",
    "# print(len([file for file in os.listdir('./processed/val/armd')]))\n",
    "# print(len([file for file in os.listdir('./processed/val/normal')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb9f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import images\n",
    "data_transfrom = transforms.Compose([  \n",
    "    transforms.ToTensor(),             \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.Resize((input_shape, input_shape)),\n",
    "])\n",
    " \n",
    "trainset = datasets.ImageFolder('./processed/train', transform = data_transfrom)\n",
    "testset = datasets.ImageFolder('./processed/val', transform = data_transfrom) \n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainset, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True, \n",
    "                                          num_workers = 1) \n",
    "testloader = torch.utils.data.DataLoader(dataset = testset, \n",
    "                                         batch_size = batch_size, \n",
    "                                         shuffle = True, \n",
    "                                         num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a9b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa37a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b613f",
   "metadata": {},
   "source": [
    "# 3. CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d77333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, in_channels, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(nn.Conv2d(in_channels = in_channels, out_channels = 9, \n",
    "                                            kernel_size = 5, padding = 2, stride = 1),\n",
    "                                  nn.BatchNorm2d(9),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.cnn2 = nn.Sequential(nn.Conv2d(in_channels = 9, out_channels = 27, \n",
    "                                            kernel_size = 5, padding = 2, stride = 1),\n",
    "                                  nn.BatchNorm2d(27),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.cnn3 = nn.Sequential(nn.Conv2d(in_channels = 27, out_channels = 81, \n",
    "                                            kernel_size = 5, padding = 2, stride = 1),\n",
    "                                  nn.BatchNorm2d(81),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(81*(input_shape//8)*(input_shape//8), num_classes)\n",
    "\n",
    "#         self.fc = nn.Linear(27*(input_shape//4)*(input_shape//4), num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.cnn3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cb71df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [6, 9, 256, 256]             684\n",
      "       BatchNorm2d-2           [6, 9, 256, 256]              18\n",
      "              ReLU-3           [6, 9, 256, 256]               0\n",
      "         MaxPool2d-4           [6, 9, 128, 128]               0\n",
      "            Conv2d-5          [6, 27, 128, 128]           6,102\n",
      "       BatchNorm2d-6          [6, 27, 128, 128]              54\n",
      "              ReLU-7          [6, 27, 128, 128]               0\n",
      "         MaxPool2d-8            [6, 27, 64, 64]               0\n",
      "            Conv2d-9            [6, 81, 64, 64]          54,756\n",
      "      BatchNorm2d-10            [6, 81, 64, 64]             162\n",
      "             ReLU-11            [6, 81, 64, 64]               0\n",
      "        MaxPool2d-12            [6, 81, 32, 32]               0\n",
      "           Linear-13                     [6, 2]         165,890\n",
      "================================================================\n",
      "Total params: 227,666\n",
      "Trainable params: 227,666\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.50\n",
      "Forward/backward pass size (MB): 202.92\n",
      "Params size (MB): 0.87\n",
      "Estimated Total Size (MB): 208.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CNN(input_shape = input_shape, in_channels = 3, num_classes = num_classes)\n",
    "summary(model, input_size = (3, input_shape, input_shape), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc2260",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9445a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09c2cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a8a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/16, average loss: 8.8779\n",
      "2/16, average loss: 8.3447\n",
      "3/16, average loss: 7.8120\n",
      "4/16, average loss: 0.5877\n",
      "5/16, average loss: 0.1948\n",
      "6/16, average loss: 0.2687\n",
      "7/16, average loss: 1.4327\n",
      "8/16, average loss: 1.8572\n",
      "9/16, average loss: 0.1463\n",
      "10/16, average loss: 0.2400\n",
      "11/16, average loss: 0.1049\n",
      "12/16, average loss: 0.0083\n",
      "13/16, average loss: 2.5463\n",
      "14/16, average loss: 0.8257\n",
      "15/16, average loss: 0.0965\n",
      "16/16, average loss: 0.0133\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss_ave=0\n",
    "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_ave += loss.item()\n",
    "#         if (batch_idx+1) % 4 == 0:\n",
    "#             print(f'{epoch+1}/{num_epochs}, {batch_idx+1}/{total_batch}: {loss.item():.4f}')\n",
    "    print(f'{epoch+1}/{num_epochs}, average loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d0f27",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32a7be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.  2.]\n",
      " [ 0. 17.]]\n",
      "Accuracy: 32/34 = 94.1 %\n",
      "Accuracy for class: ARMD is 88.2 %\n",
      "Accuracy for class: Normal is 100.0 %\n",
      "Precision: 100.0 %\n",
      "recall: 88.2 %\n",
      "F1-score: 93.8 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "conf_mat = np.zeros((num_classes, num_classes))\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        out = model(images)\n",
    "        predictions = torch.argmax(out, 1) \n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            conf_mat[label, prediction] += 1\n",
    "print(conf_mat)\n",
    "\n",
    "accuracy = sum(correct_pred.values())/sum(total_pred.values())\n",
    "print(f'Accuracy: {sum(correct_pred.values())}/{sum(total_pred.values())} = {100*accuracy:.1f} %')\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:3s} is {accuracy:.1f} %')\n",
    "    \n",
    "precision = conf_mat[0,0]/(conf_mat[0,0] + conf_mat[1,0])\n",
    "recall = conf_mat[0,0]/(conf_mat[0,0] + conf_mat[0,1])\n",
    "print(f'Precision: {precision*100:.1f} %')\n",
    "print(f'recall: {recall*100:.1f} %')\n",
    "\n",
    "\n",
    "# print F1-score\n",
    "f1_score = conf_mat[0,0]/(conf_mat[0,0]+0.5*(conf_mat[0,1]+conf_mat[1,0]))\n",
    "print(f'F1-score: {f1_score*100:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c5760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
